## Streamlit Application to automate Pre-processing of text data

import time
start = time.time()
import streamlit as st

# Initializing all libraries
import pandas as pd
import numpy as np

# For manipulating text data
import nltk
import re
nltk.download('omw-1.4')
nltk.download('stopwords')
import contractions
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer  
lemma = WordNetLemmatizer()
from gensim.parsing.preprocessing import strip_punctuation, strip_tags, strip_numeric

# For visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px               
from plotly.subplots import make_subplots 
import plotly.graph_objects as go

# For language conversion
from langdetect import detect
from deep_translator import GoogleTranslator

# load nltk's English stopwords as variable called 'stopwords'
stopwords = nltk.corpus.stopwords.words('english')

# load nltk's SnowballStemmer as variabled 'stemmer'
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english")

from sklearn.feature_extraction.text import CountVectorizer

# NLTK Stop words extended
stopwords.extend(['from', 'subject', 're', 'edu', 'use'])
stopwords.extend(["'d", "'ll", "'re", "'s", "'ve", 'could', 'might', 'must', "n't", 'need', 'sha', 'wo', 'would'])

# Initializing custom functions

def language_translater(text):
    translated = GoogleTranslator(source=detect(text), target='en').translate(text)
    return translated

def replace_all(text, dic):
    for i, j in dic.items():
        text = text.replace(str(i), j + " ")
    return text

def textClean(text):
    cleantext = " ".join([lemma.lemmatize(word.lower()) for word in strip_numeric(strip_punctuation(text)).split()])
    return cleantext

def tokenize_only(text):
    # Tokenizing each sentence and then word
    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.lower() not in stopwords]
    
    filtered_tokens = []
    
    # Filtering out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)
    return filtered_tokens

def tokenize_and_stem(text):
    filtered_tokens = tokenize_only(text)
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems

from wordcloud import WordCloud
def create_word_cloud(final_data, title):
    wordcloud = WordCloud(width=1600, height=800, max_font_size=200, stopwords = stopwords,
                          background_color='white').generate(final_data)
    
    # plt the image generated by WordCloud class
    plt.figure(figsize=(12,10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.title(title+"\n", fontsize = 16)
    plt.show()
    
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

# define unit func to process one doc
from nltk import sent_tokenize, word_tokenize
def vader_unit_func(doc0,column_name):
    sents_list0 = sent_tokenize(doc0)
    vs_doc0 = []
    sent_ind = []
    for i in range(len(sents_list0)):
        vs_sent0 = analyzer.polarity_scores(sents_list0[i])
        vs_doc0.append(vs_sent0)
        sent_ind.append(i)
        
    # obtain output as DF    
    doc0_df = pd.DataFrame(vs_doc0)
    doc0_df.columns = [x+column_name for x in doc0_df.columns]
    doc0_df.insert(0, 'sent_index', sent_ind)  # insert sent index
    doc0_df.insert(doc0_df.shape[1], 'sentence', sents_list0)
    return(doc0_df)

# define wrapper func
def vader_wrap_func(corpus0,column_name):
    
    # use ifinstance() to check & convert input to DF
    if isinstance(corpus0, list):
        corpus0 = pd.DataFrame({'text':corpus0})
    
    # define empty DF to concat unit func output to
    vs_df = pd.DataFrame()    
    
    # apply unit-func to each doc & loop over all docs
    for i1 in range(len(corpus0)):
        doc0 = str(corpus0.iloc[i1])
        vs_doc_df = vader_unit_func(doc0,column_name)  # applying unit-func
        vs_doc_df.insert(0, 'doc_index', i1)  # inserting doc index
        vs_df = pd.concat([vs_df, vs_doc_df], axis=0)
        
    return(vs_df)

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):   
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    vif['VIF'].sort_values()
    return(vif)


# SETTING PAGE CONFIG TO WIDE MODE AND ADDING A TITLE AND PAGE ICON
st.set_page_config(layout="wide", page_icon=":taxi:")

logo = st.container()
header = st.container()
dataset = st.container()
understanding = st.container()
trans_emoticon = st.container()
preprocessing = st.container()
ngram = st.container()
features = st.container()
model = st.container()

with logo:
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    #background = st.image("https://github.com/Kenrich005/Uber_reviews_textanalytics/blob/6ace3968785f4cfcaca57fbc589238935940bc86/ISB_Logo_JPEG.jpg?raw=true")
    col6.image("https://github.com/Kenrich005/Uber_reviews_textanalytics/blob/6ace3968785f4cfcaca57fbc589238935940bc86/ISB_Logo_JPEG.jpg?raw=true", width=200)

with header:
    
    st.title('Application to Automate Pre-processing of Text-based data')
    st.markdown("""---""")
    st.markdown('#### Expectations')
    st.markdown(''' 
    - Read Uber text reviews data  
    - Translate Reviews from different languages to English  
    - Convert Emoticon unicode to its respective description''')  
    

with dataset:
    st.markdown('''Let's input the dataset  
    For this exercise, we are using Uber reviews itunes file  
    Click on the link to see the file: https://raw.githubusercontent.com/Kenrich005/Uber_reviews_textanalytics/main/uber_reviews_itune.csv''')
    
    df = pd.read_csv('https://raw.githubusercontent.com/Kenrich005/Uber_reviews_textanalytics/main/uber_reviews_itune.csv', encoding='cp1252')
    st.write(df.head(5))
    
    
with understanding:
    st.subheader("Understanding the data")
    st.markdown("""
    We begin our journey with understand the data given to us""")
    st.write(df.describe(include = 'object').T)
    
    tab1, tab2, tab3 = st.tabs(["Rating", "App_Version", "Language"])
    
    with tab1:
        # Rating distribution
        st.markdown("#### Rating' distribution")
        st.markdown("###### How have the customer's rated us?")
        df_rating_count  = df.groupby(by=["Rating"]).size().reset_index(name="Counts")
        fig_bar1 = px.bar(data_frame=df_rating_count, x="Rating", y="Counts", text_auto=True, width=600, height=400,color_discrete_sequence =['#0067A0']*3)
        st.plotly_chart(fig_bar1)
    
    with tab2:
        # App_version distribution
        st.markdown("#### App_version' distribution")
        st.markdown("###### How spread is the App_version?")
        df_rating_count  = df.groupby(by=["App_Version"]).size().reset_index(name="Counts")
        fig_bar1 = px.bar(data_frame=df_rating_count, x="App_Version", y="Counts", text_auto=True, width=600, height=400,color_discrete_sequence =['#0067A0']*3)
        st.plotly_chart(fig_bar1)
    
    with tab3:
        st.markdown("#### Language distribution")
        st.markdown("###### How have the customer's rated us?")
        df['Language'] = df.Review.apply(lambda text: detect(text))
        df_rating_count  = df.groupby(by=["Language"]).size().reset_index(name="Counts")
        fig_bar1 = px.bar(data_frame=df_rating_count, x="Language", y="Counts", text_auto=True, width=600, height=400,color_discrete_sequence =['#0067A0']*3)
        st.plotly_chart(fig_bar1)
        
    


with trans_emoticon:
    st.subheader("""Handling Emoticons""")
    st.markdown("""On preliminary analysis of the dataset, multiple languages and emoticons were detected  
    Review column was translated to 'English' in order to maintain consistency  
    In this stage we will replace all emoticons with their respective description
    - """)
    
    st.markdown(""" ### Reading emojis description dictionary""")
    df_emojis=pd.read_csv("https://raw.githubusercontent.com/Kenrich005/Uber_reviews_textanalytics/main/emoji_description.csv")
    df_emojis['Code']=df_emojis['Code'].str.replace('+','+000')
    st.write(df_emojis.head())
    
    st.markdown("##### Original Sentence")
    st.write(df.Review[1])
    
    # Making two lists, 1. emoji code, 2. emoji meaning
    to_replace = df_emojis.Code.tolist()
    replace_with = df_emojis['CLDR Short Name'].tolist()

    # Using zip() to convert lists to dictionary
    res = dict(zip(to_replace, replace_with))
    
    df.Review = df.Review.apply(lambda text: replace_all(text, res))
    df.Title = df.Title.apply(lambda text: replace_all(text, res))

    st.markdown("##### After emoticon replacement")
    st.write(df.Review[1])
    
    df['Review'] = df.Review.apply(lambda text: contractions.fix(text))
    
    st.markdown("##### After contraction words replacement")
    st.write(df.Review[1])
    
    df['Clean_Review'] = df.Review.apply(lambda text: textClean(text))
    
    st.markdown("##### After Removing punctuation and lemmatizing the words")
    st.write(df.Review[1])
    
    
    
    tokenized_text = []
    for i in range(len(df)):
        tokenized_text.extend(tokenize_only(df.Clean_Review[i]))
        
         
    st.subheader("Uber Reviews - most used words (Unigrams)")
    
    wordcloud = WordCloud(width = 500, height = 400, background_color = 'white').generate(' '.join(tokenized_text))
    fig = plt.figure()
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.show()
    st.pyplot(fig)

    st.subheader("Barplot of top 40 used words in all the Reviews ")
    df_tokens = pd.DataFrame(tokenized_text).value_counts().rename_axis('tokens').reset_index(name='count')
    fig_bar1 = px.bar(data_frame=df_tokens.head(40), x="tokens", y="count", text_auto=True, width=1500, height=600,color_discrete_sequence =['#0067A0']*40)
    st.plotly_chart(fig_bar1)
